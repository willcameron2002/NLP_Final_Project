{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5dcaee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Interaction Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LakeShowYo</td>\n",
       "      <td>2023-09-27T22:26:53.000Z</td>\n",
       "      <td>“Giannis drives… kicks it out… Dame from the l...</td>\n",
       "      <td>2500</td>\n",
       "      <td>27000</td>\n",
       "      <td>29500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClutchPoints</td>\n",
       "      <td>2023-09-27T23:54:36.000Z</td>\n",
       "      <td>\"It's a perfect situation for Damian Lillard.....</td>\n",
       "      <td>97</td>\n",
       "      <td>580</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zay</td>\n",
       "      <td>2023-09-27T23:17:15.000Z</td>\n",
       "      <td>Give me this 5 over Dame &amp; Giannis Bucks</td>\n",
       "      <td>17</td>\n",
       "      <td>145</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w4yst</td>\n",
       "      <td>2023-09-27T22:36:48.000Z</td>\n",
       "      <td>DAME TO THE BUCKS NIGGA</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Santi</td>\n",
       "      <td>2023-09-27T23:54:41.000Z</td>\n",
       "      <td>Heat are front runners for Beal\\n\\nDon’t make ...</td>\n",
       "      <td>42</td>\n",
       "      <td>178</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>Blake Schofield</td>\n",
       "      <td>2023-09-27T20:41:37.000Z</td>\n",
       "      <td>We will see if the Bucks can get Lillard to sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>Derek Tadashiroll</td>\n",
       "      <td>2023-09-27T18:40:13.000Z</td>\n",
       "      <td>The Bucks are going to get Lillard to turn up ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>Gedi</td>\n",
       "      <td>2023-09-27T19:19:16.000Z</td>\n",
       "      <td>Permitter defense is non-existent, but it’s go...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>John Lopez</td>\n",
       "      <td>2023-09-27T18:51:39.000Z</td>\n",
       "      <td>I feel like trading holiday to get Lillard at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>Knowledge</td>\n",
       "      <td>2023-09-27T21:00:00.000Z</td>\n",
       "      <td>@Bucks\\n are worse now. All they needed was to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1463 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Username                      Time  \\\n",
       "0            LakeShowYo  2023-09-27T22:26:53.000Z   \n",
       "1          ClutchPoints  2023-09-27T23:54:36.000Z   \n",
       "2                   Zay  2023-09-27T23:17:15.000Z   \n",
       "3                 w4yst  2023-09-27T22:36:48.000Z   \n",
       "4                 Santi  2023-09-27T23:54:41.000Z   \n",
       "...                 ...                       ...   \n",
       "1458    Blake Schofield  2023-09-27T20:41:37.000Z   \n",
       "1459  Derek Tadashiroll  2023-09-27T18:40:13.000Z   \n",
       "1460               Gedi  2023-09-27T19:19:16.000Z   \n",
       "1461         John Lopez  2023-09-27T18:51:39.000Z   \n",
       "1462          Knowledge  2023-09-27T21:00:00.000Z   \n",
       "\n",
       "                                                  Tweet  Retweets  Likes  \\\n",
       "0     “Giannis drives… kicks it out… Dame from the l...      2500  27000   \n",
       "1     \"It's a perfect situation for Damian Lillard.....        97    580   \n",
       "2              Give me this 5 over Dame & Giannis Bucks        17    145   \n",
       "3                               DAME TO THE BUCKS NIGGA         0      5   \n",
       "4     Heat are front runners for Beal\\n\\nDon’t make ...        42    178   \n",
       "...                                                 ...       ...    ...   \n",
       "1458  We will see if the Bucks can get Lillard to sh...         0      1   \n",
       "1459  The Bucks are going to get Lillard to turn up ...         0      0   \n",
       "1460  Permitter defense is non-existent, but it’s go...         1      0   \n",
       "1461  I feel like trading holiday to get Lillard at ...         0      2   \n",
       "1462  @Bucks\\n are worse now. All they needed was to...         0      0   \n",
       "\n",
       "      Interaction Count  \n",
       "0                 29500  \n",
       "1                   677  \n",
       "2                   162  \n",
       "3                    15  \n",
       "4                   220  \n",
       "...                 ...  \n",
       "1458                 11  \n",
       "1459                 10  \n",
       "1460                 11  \n",
       "1461                 12  \n",
       "1462                 10  \n",
       "\n",
       "[1463 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('FINAL DATASETS\\Lillard Trade Final.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828ffa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import re\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# Pretrained\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(re.sub('\\n',' ',t))\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def ClassifyTweets(dataframe):\n",
    "    sentiment = []\n",
    "    for tweet in dataframe['Tweet']:\n",
    "        text = preprocess(tweet)\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        ranking = np.argsort(scores)\n",
    "        sentiment.append(config.id2label[ranking[-1]])\n",
    "    dataframe['Sentiment'] = sentiment\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf6deaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A better fit than Damian lillard, make it HAPPEN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dame + Giannis is going to be INSANE to watch....</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment  Subject\n",
       "0   A better fit than Damian lillard, make it HAPPEN          1        0\n",
       "1  Dame + Giannis is going to be INSANE to watch....          2        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "key = pd.read_csv('FINAL DATASETS/Lillard Final Key.csv')\n",
    "pred = pd.DataFrame(key['Tweet'])\n",
    "key_copy = key.copy()\n",
    "fix = {0:'negative',1:'neutral',2:'positive'}\n",
    "key_copy['Sentiment'] = key_copy['Sentiment'].map(fix)\n",
    "key.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e03f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ClassifyTweets(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a7c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_score(key_copy['Sentiment'].values,pred['Sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af7d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca83ceb869e4142b38bfe9f1f431654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234bebb7f2a04cd88bd6973c965d782e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "key = pd.read_csv('FINAL DATASETS/Lillard Final Key.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(key['Tweet'], key['Sentiment'], test_size = 1/6, random_state = 42)\n",
    "train = pd.DataFrame(zip(X_train, y_train), columns = ['text','label'])\n",
    "test = pd.DataFrame(zip(X_test, y_test), columns = ['text','label'])\n",
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", max_length = 514,truncation=True)\n",
    "\n",
    "tokenized_train = train.map(tokenize_function, batched = True)\n",
    "tokenized_test = test.map(tokenize_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c73e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir = 'Test_trainer',num_train_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7da15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d826dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits,axis=1)\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2a85ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 1:29:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=0.3311675540984623, metrics={'train_runtime': 5371.4303, 'train_samples_per_second': 0.465, 'train_steps_per_second': 0.059, 'total_flos': 660353011290000.0, 'train_loss': 0.3311675540984623, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                 args=training_args,\n",
    "                 train_dataset=tokenized_train,\n",
    "                 eval_dataset=tokenized_test,\n",
    "                 compute_metrics = compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4017930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 09:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6794116497039795,\n",
       " 'eval_accuracy': 0.72,\n",
       " 'eval_runtime': 96.5774,\n",
       " 'eval_samples_per_second': 1.035,\n",
       " 'eval_steps_per_second': 0.135,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hopefully less than 0.245 for loss, over 70% accuracy eval\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85feda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(r'C:\\Users\\Owner\\Desktop\\NLPprojectData\\HuggingFace Models\\5 epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
